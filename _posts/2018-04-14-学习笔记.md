---
layout:     post                    # 使用的布局（不需要改）
title:      学习笔记         # 标题
date:       2018-4-14             # 时间
author:     Kiri                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 笔记
---

# 卷积神经网络

## LeNet

Input Layer: $1\times32\times32$

Conv1 Layer: kernal size: $$5\times5$$

Pooling Layer: Average Pooling, size: $$2\times2$$, sigmoid

Conv3 Layer: kernal size: $$5\times5$$

Pooling Layer: Average Pooling,  size: $$2\times2$$, sigmoid

Conv5 Layer: kernal size: $$5\times 5$$

Fully Connected Layer: sigmoid

Output Layer: Gaussian connection

## AlexNet

Input Layer: $$3\times 227\times227$$

Conv1 Layer: kernal size: $11\times 11\times 3$, stride: $4$

Pooling Layer: Max Pooling, size: $5 \times 5$, stride: 4, ReLU

Conv2 Layer: 

## GoogleNet

## VGG



## ResNet

# 目标检测

## R-CNN

利用Selective Search提取约2000个候选区域，选取卷积神经网络AlexNet模型对每个候选区域提取特征，并将提取的特征送入每一类SVM 分类器判断是否属于该类，最后使用回归器精细修正候选框位置。

## Fast R-CNN

Fast RCNN将边界框回归直接加入到CNN网络中训练，完全替代SVM，实现了特征提取和分类定位的融合，极大地提高了训练速度和检测精度。


## Faster R-CNN
在这个网络中，引入了RPN（Region Proposal Network），与整个检测网络共享整张图的卷积特征。PRN是一个Full-Convolutional网络，可以同时预测每个位置物体边界位置和目标物体的score。在PASCAL VOC 2007上mAP为73.2%。

[TF-Faster-RCNN](https://caiwendi.github.io/2018/06/26/FasterRCNN-TF%E7%A8%8B%E5%BA%8F%E7%AC%94%E8%AE%B0/)程序运行笔记。

## Mask R-CNN

Mask R-CNN是一个小巧、灵活的通用对象实例分割框架。它不仅可对图像中的目标进行检测，还可以对每一个目标给出一个高质量的分割结果。它在Faster R-CNN基础之上进行扩展，并行地在Bounding Box Recognition分支上添加一个用于预测目标Mask的新分支。该网络还很容易扩展到其他任务中，比如估计人的姿势，也就是Person Keypoint Detection。

## YOLO

与基于分类器的方法不同，YOLO将目标检测作为回归问题求解，用单个神经网络在一次评估中直接从整幅图像中预测边界框与类别概率。YOLO只有一个训练网络，大大加快了检测的速度。

## SSD

SSD采用VGG-16作为基础模型，然后在VGG-16的基础上新增了卷积层来获得更多的特征图以用于检测。


# 机器学习

## 梯度下降算法

$$
h_\theta(x)=\theta_0x_0+\theta_1x_1+\cdots+\theta_nx_n=\sum_{i=0}^n \theta_ix_i=\theta^TX
$$

$$
J(\theta)=\frac12\sum_{i=1}^n(h_\theta(x)-y)
$$

$$\nabla J(\theta)=\cfrac{\partial J(\theta)}{\partial\theta_j}=\cfrac{\partial}{\partial\theta_j}\cfrac12\sum_{i-1}^n(h_\theta(x)-y)^2=(h_\theta(x)-y)x_j​$$

$$\theta_j:=\theta_j-\alpha(h_\theta(x)-y)x_j​$$

### 随机梯度下降

每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。

### 小批量梯度下降

是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

## 梯度下降优化算法

### Momentum

$$
v_t = \gamma v_t + \eta \nabla_\theta J(\theta) \\
\theta = \theta - v_t
$$

动量因子$\gamma​$通常设置为0.9或相近值，参数更新方向不仅由当前梯度决定， 也与此前累计下降方向相关。

## 生成学习算法

### 高斯判别分析法

**多变量高斯分布** $$Z\sim \mathcal N(\vec{\mu},\Sigma)​$$ 其中均值$$\vec{\mu}\in \mathbb{R}^n​$$，方差$$\Sigma \in \mathbb{R}^{n \times n}​$$。

$$p(x;\mu,\Sigma)=\cfrac{1}{(2\pi)^{\frac n2}\left \vert \Sigma\right \vert ^{\frac n2}}exp(-\cfrac 12(x-\mu)^T\Sigma^{-1}(x-\mu))​$$

**高斯判别分析模型**

$$y \sim Bernouli(\phi)$$	                 $$p(y)=\phi^y(1-\phi)^{1-y}$$              $$y$$服从关于$$\phi$$的伯努利分布

$$x \vert y=0 \sim \mathcal N(\mu_0,\Sigma)$$            $$p(x \vert y=0)=\cfrac1{(2\pi)^{\frac n2}\left \vert \Sigma\right \vert ^{\frac 12}}exp(-\cfrac 12(x-\mu_0)^T\Sigma^{-1}(x-\mu_0))$$

$$x \vert y=1 \sim \mathcal N(\mu_1,\Sigma)​$$            $$p(x \vert y=1)=\cfrac1{(2\pi)^{\frac n2}\left \vert \Sigma\right \vert ^{\frac 12}}exp(-\cfrac 12(x-\mu_1)^T\Sigma^{-1}(x-\mu_1))​$$

$$joint \ likelyhood​$$:

$$\begin{align*} \ell(\phi,\mu_0,\mu_1,\Sigma)&=\log\prod_{i=1}^mp(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma)\\&=\log\prod_{i=1}^mp(x^{(i)} \vert y^{(i)};\phi,\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi) \end{align*}$$

使$$joint\ likelyhood​$$ $$\ell​$$取最大时，参数$$\phi,\mu_0,\mu_1,\Sigma​$$如下

$$\phi=\cfrac 1m\sum_{i=1}^m1\{y^{(i)}=1\}​$$

$$\mu_0=\frac{\sum_{i=1}^m1\{y^{(i)}=0\}x^{(i)}}{\sum_{i=1}^m1\{y^{(i)}=0\}}$$         $$1\{y^{(i)}=0\}x^{(i)}$$标签为0对应的$$x^{(i)}$$求和        $$1\{y^{(i)}=0\}$$标签为0的数目

$$\mu_1=\frac{\sum_{i=1}^m1\{y^{(i)}=1\}x^{(i)}}{\sum_{i=1}^m1\{y^{(i)}=1\}}$$

$$\Sigma=\cfrac 1m\sum_{i=1}^m(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T$$

$$p(x \vert y)$$服从多变量高斯分布，$$p(y \vert x)$$必然遵循逻辑函数。

### 朴素贝叶斯











<html>
<head>
<title>MathJax TeX Test Page</title>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>
</head>
<body>
