---
layout:     post                    # 使用的布局（不需要改）
title:      FCOS Fully Convolutional One-Stage Object Detection           # 标题 
subtitle:   论文阅读笔记 #副标题
date:       2019-05-10              # 时间
author:     Kiri                      # 作者
header-img: img/archives-bg-mayday.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Detection
---

# 阅读笔记

## 论文相关信息

**题目：**FCOS: Fully Convolutional One-Stage Object Detection

**作者：**Zhi Tian, Chunhua Shen, Hao Chen, Tong He

**出处：**<https://arxiv.org/pdf/1904.01355.pdf>

---

## 摘要

本文提出了一种全卷积的单阶段目标检测（FCOS）方法，以逐像素预测的方式解决目标检测方法，类似于语义分割。目前绝大多数先进的目标检测方法都取决于预先定义好的Anchor boxes，与之相反，本文的FCOS无需定义Anchor boxes以及proposal（候选框）。FCOS由于无需定义Anchors，因此避免了与Anchor相关的复杂计算，例如计算在训练过程中的重叠比，这明显减少了训练过程中内存的占用，更重要的是避免了与Anchor相关的超参数的设定，最终的检测效果对这些参数的设定十分敏感。仅凭借唯一的后处理方法——NMS（非最大抑制），本文提出的目标检测算法FCOS比以往的单阶段目标检测算法更具简单的优势，并提高了检测精度。

---

## 相关工作

- **基于Anchor的目标检测算法：**Faster R-CNN，SSD，YOLOv2等
- **没有Anchor的目标检测算法：**YOLOv2

---

## FCOS

**网络结构：**

![pic1](https://github.com/caiwendi/caiwendi.github.io/raw/master/img/FCOS-1.png)

C3，C4，C5表示骨干卷积神经网络中间层输出的特征图，P3到P7则表示用于最终预测的特征层次。$H \times W$表示特征图的大小，$s$为特征图的采样率，其中center-ness用于降低低质量的目标检测边框，提升了整体的表现。

文中的骨干网络采用ResNeXt-32x8d-101-FPN的FCOS网络AP表现明显优于其他算法。

对于特征图中的每一个位置$(x,y)$可以映射到原图像上为$([\frac{s}{2}] +xs,\frac{s}{2}] +ys)$，更近一步的，如果$(x,y)$落到任意一个GT中，则判定其为正例，并将其分类标签$c^*$设置与GT一致；若没落在任一GT中，则为负例，并将分类标签设置为0，表示为背景。

GT为$B_i=(x_0^{(i)},y_0^{(i)},x_1^{(i)},y_1^{(i)},c^{(i)})$，表示单张图像中第i个目标的标签，$(x_0^{(i)},y_0^{(i)})$为GT的右上角点坐标，(x_1^{(i)},y_1^{(i)})$为GT的左下角点坐标，$$c^{(i)}$为该目标的类标签

将一个四维向量${\bf t}^*=(l^*,t^*,r^*,b^*)$作为每个samples回归的目标，具体计算如下
$$
l^*=x-x_o^{(i)} \\
t^*=y-y_0^{(i)} \\
r^*=x_1^{(i)}-x \\
b^*=y_1^{(i)}-y
$$
**网络输出：**

由于该网络是在COCO数据集上进行训练的，因此网络输出一个80维的向量$\bf p$作为分类标签，以及一个4维的的向量${\bf t}=(l,t,r,b)$作为bbox的坐标。

**损失函数：**
$$
L(\{{\bf p}_{x,y}\},\{{\bf t}_{x,y}\}) =\cfrac{1}{N_{pos}}\sum_{x,y}L_{cls}({\bf p}_{x,y},c^*_{x,y})+\cfrac{\lambda}{N_{pos}}\sum_{x,y}l_{\{c_{x,y}^*>0\}}L_{reg}({\bf t}_{x,y},{\bf t}_{x,y}^*)\\
l_{\{c_{x,y}^*>0\}}=\begin{cases}1 \qquad \text{if} \quad c_{x,y}^*>0 \\0 \qquad \text{otherwise} \end{cases}
$$
文中提到将$ p_{x,y}>0.05$的位置设为正例，但是未给出$ p_{x,y}$是如何得到的

**采用FPN的多尺度特征进行FCOS的预测：**

与基于Anchor的目标检测算法在不同尺度的特征图上采用不同尺度的Anchor做法不一样，FCOS中首先根据各个特征图计算回归的目标值$(l^*,t^*,r^*,b^*)$，接下来，如果$\max(l^*,t^*,r^*,b^*)>m_i$或者$\max(l^*,t^*,r^*,b^*)<m_{i-1}$，将其设置为负例，不用做第$i$层特征图的回归值，在本文中$m_1,m_2,m_3,m_4,m_5,m_6,m_7=0,64,128,256,512, \infty$。

**Center-ness：**

在分类分支的最后加上一个平行的分支用于预测“center-ness”，其值为0到1之间。

---

## 实验

**训练集：**COCO

**优化算法：**SGD；initial lr=0.01，reduce 10  at 60k iterate ，80 iterate；weight decay 0.0001；Momentum 0.9

**其他参数：**后处理参数与RetinaNet一致，如NMS的阈值等。

<html>

<head>
<title>MathJax TeX Test Page</title>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>
</head>
<body>