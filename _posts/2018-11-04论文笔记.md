---
layout:     post                    # 使用的布局（不需要改）
title:      Identity Mappings in Deep Residual Networks           # 标题 
subtitle:   Matplotlib operations #副标题
date:       2018-11-04              # 时间
author:     Brian                      # 作者
header-img: img/Taylor1.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 笔记
---



# 论文笔记

**论文题目：**$Identity Mappings in Deep Residual Networks$

### 1、Introduction

- 一般残差单元过程可以公式化为$$x_{l+1}=f(h(x_l)+F(x_l,W_l))$$。$$x_l$$和$x_{l+1}$为第$l$层的输入和输出。在ResNet V1中$$h(x_l)=x_l$$为恒等映射，$f$为**ReLU**函数。
- 采用恒等映射的收敛速度最快，训练的错误率下降最快
- 使用“**纯净**”的信息路径有助于更易于优化
- 与ResNet V1采用的“Post-activation”不同，ResNet V2采用的“Pre-activation”
  - Post-activation：weight $\rightarrow$ BN $\rightarrow $ ReLU $\rightarrow $ weight $\rightarrow $ BN $\rightarrow $ addition $\rightarrow $ ReLU
  - Pre-activation：BN $\rightarrow $ ReLU $\rightarrow $ weight $\rightarrow $ BN $\rightarrow $ ReLU $\rightarrow $ weight $\rightarrow $ addition
- 



















<html>

<head>
<title>MathJax TeX Test Page</title>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>
</head>
<body>