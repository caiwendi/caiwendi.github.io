---
layout:     post                    # 使用的布局（不需要改）
title:      学习笔记         # 标题 
date:       2018-4-14             # 时间
author:     Kiri                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 学习
---

# 卷积神经网络

## LeNet

Input Layer: $1\times32\times32$图像

Conv1 Layer: 包含6个卷积核，kernal size: $$5\times5$$

Pooling Layer: Average Pooling, size: $$2\times2$$	激活函数：sigmoid

Conv3 Layer: 包含16个卷积核，kernal size: $$5\times5$$

Pooling Layer: Average Pooling,  size: $$2\times2$$	激活函数：sigmoid

Conv5 Layer: 包含120个卷积核，kernal size: $$5\times 5$$

Fully Connected Layer: 激活函数：sigmoid

Output Layer: Gaussian connection





## AlexNet

Input Layer: $$3\times 227\times227$$图像



## GoogleNet

## VGG

## ResNet

# 目标检测

## R-CNN

##Fast R-CNN

## Faster R-CNN

## YOLO

## SSD

# 机器学习

##梯度下降法

$$h_\theta(x)=\theta_0x_0+\theta_1x_1+\cdots+\theta_nx_n=\sum_{i=0}^n \theta_ix_i=\theta^TX$$

$$J(\theta)=\frac12\sum_{i=1}^n(h_\theta(x)-y)$$

$$\nabla J(\theta)=\frac{\partial J(\theta)}{\partial\theta_j}=\frac{\partial}{\partial\theta_j}\frac12\sum_{i-1}^n(h_\theta(x)-y)^2=(h_\theta(x)-y)x_j$$

$$\theta_j:=\theta_j-\alpha(h_\theta(x)-y)x_j$$

### 随机梯度下降

每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。

### 小批量梯度下降

是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

## 生成学习算法

### 高斯判别分析法

**多变量高斯分布** $$Z\sim \mathcal N(\vec{\mu},\Sigma)$$ 其中均值$$\vec{\mu}\in \mathbb{R}^n$$，方差$$\Sigma \in \mathbb{R}^{n \times n}$$。

$$p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{\frac n2}\left|\Sigma\right|^{\frac n2}}exp(-\frac 12(x-\mu)^T\Sigma^{-1}(x-\mu))$$

**高斯判别分析模型**

$$y \sim Bernouli(\phi)$$	                 $$p(y)=\phi^y(1-\phi)^{1-y}$$              $$y$$服从关于$$\phi$$的伯努利分布

$$x|y=0 \sim \mathcal N(\mu_0,\Sigma)$$            $$p(x|y=0)=\frac1{(2\pi)^{\frac n2}\left|\Sigma\right|^{\frac 12}}exp(-\frac 12(x-\mu_0)^T\Sigma^{-1}(x-\mu_0))$$

$$x|y=1 \sim \mathcal N(\mu_1,\Sigma)$$            $$p(x|y=1)=\frac1{(2\pi)^{\frac n2}\left|\Sigma\right|^{\frac 12}}exp(-\frac 12(x-\mu_1)^T\Sigma^{-1}(x-\mu_1))$$

$$joint \ likelyhood$$:

$$\begin{align*} \ell(\phi,\mu_0,\mu_1,\Sigma)&=log\prod_{i=1}^mp(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma)\\&=log\prod_{i=1}^mp(x^{(i)}|y^{(i)};\phi,\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi) \end{align*}$$

使$$joint\ likelyhood$$ $$\ell$$取最大时，参数$$\phi,\mu_0,\mu_1,\Sigma$$如下

$$\phi=\frac 1m\sum_{i=1}^m1\{y^{(i)}=1\}$$

$$\mu_0=\frac{\sum_{i=1}^m1\{y^{(i)}=0\}x^{(i)}}{\sum_{i=1}^m1\{y^{(i)}=0\}}$$         $$1\{y^{(i)}=0\}x^{(i)}$$标签为0对应的$$x^{(i)}$$求和        $$1\{y^{(i)}=0\}$$标签为0的数目

$$\mu_1=\frac{\sum_{i=1}^m1\{y^{(i)}=1\}x^{(i)}}{\sum_{i=1}^m1\{y^{(i)}=1\}}$$

$$\Sigma=\frac 1m\sum_{i=1}^m(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T$$

$$p(x|y)​$$服从多变量高斯分布，$$p(y|x)​$$必然遵循逻辑函数。

### 朴素贝叶斯











<html>
<head>
<title>MathJax TeX Test Page</title>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>
</head>
<body>







