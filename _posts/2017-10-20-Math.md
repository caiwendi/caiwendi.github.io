---
layout:     post                    # 使用的布局（不需要改）
title:      Math               # 标题 
subtitle:   Math #副标题
date:       2017-11-01              # 时间
author:     Brian                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 研究方向
<script  type="text/javascript" async  src="<https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML>"></script>
---
  A simplified QO formulation with shared pairwise terms short for QO$^{sw}$ is now described which is significantly faster in practice than the one described above. By $A_{p_i,p_j}(l_i,l_j)$, the pairwise energy term is denoted for pixel $p_i$ taking the label $l_i$, and pixel $p_j$ taking the label $l_j$. In this section, a Potts-type pairwise model is proposed to describe by the following equation

​     The pairwise terms are shared by different pairs of classes and only $P\times P$ terms are needed to learn. To derive the inference and gradient equations after this simplification, the inference equation $(A+\lambda I)x=B$ is rewrited as

​     where $x_k$ ，denotes the vector of scores for all the pixels for the class $k\in\{1,\cdots,L\}$. The per-class unaries are denoted by $b_k$ , and the pairwise terms $\hat{A}$  are shared between each pair of classes. The equations that follow are derived by specializing the general inference (Eq.2) and gradient equations (Eq. 3, 4) to this particular setting. Following simple manipulations, the inference procedure becomes a two step process where we first compute the sum of our scores $\sum\nolimits_{i}^{}x_i$ , followed by $x_k$ . the scores for the class $k$  as

$$(\lambda I+(L-1)\hat{A})\sum_{i}^{}x_i=\sum_{i}^{}b_i,$$

$$(\lambda I-\hat{A})x_k=b_k-\hat{A}\sum_{i}^{}x_i.$$

Derivatives of the unary terms with respect to the loss are obtained by solving

$$(\lambda I+(L-1)\hat{A})\sum_{i}^{}\frac{\partial \zeta}{\partial b_i}=\sum_{i}^{}\frac{\partial \zeta}{\partial x_i},$$

$$(\lambda I-\hat{A})\frac{\partial \zeta}{\partial b_k}=\frac{\partial \zeta}{\partial x_k}-\hat{A}\sum_{i}^{}\frac{\partial \zeta}{\partial b_i}.$$

Finally, the gradients of $\hat{A}$  are computed as

$$\frac{\partial \zeta}{\partial \hat{A}}=\sum_{k}^{}\frac{\partial \zeta}{\partial b_k}\bigotimes \sum_{i\neq k}^{}x_i.$$







